**举个例子**

以机器翻译为例

```python
# Encoder
输入序列：["我", "爱", "自然语言处理"]
↓
词嵌入 + 位置编码 → [向量1, 向量2, 向量3]
↓
经过6个编码器层的处理：
   每个层包含：
   1. 多头自注意力（关注整个输入序列）
   2. 前馈神经网络（特征变换）
   3. 残差连接 + 层归一化
↓
输出上下文表示：包含"我-爱-处理"关系的综合特征矩阵

# Decoder
已生成部分：["I"]
↓
输入：["<start>", "I"]（起始符 + 已生成词）
↓
经过6个解码器层的处理：
   每个层包含：
   1. 掩码多头注意力（仅关注已生成部分）
   2. 编码-解码注意力（连接编码器输出）
   3. 前馈神经网络
   4. 残差连接 + 层归一化
↓ 
预测下一个词："love"
```

![解码器内部结构](https://jalammar.github.io/images/t/transformer_decoding_1.gif)

